# Code to define discriminator
def define_discriminator(in_shape=(28,28,1)):
  model = Sequential()
  model.add(Conv2D(64,(3,3),strides=(2,2),padding=&#39;same&#39;,
  input_shape=in_shape))
  model.add(LeakyReLU(alpha=0.2))
  model.add(Dropout(0.4))
  model.add(Conv2D(64, (3,3), strides=(2, 2), padding=&#39;same&#39;))
  model.add(LeakyReLU(alpha=0.2))
  model.add(Dropout(0.4))
  model.add(Flatten())
  model.add(Dense(1, activation=&#39;sigmoid&#39;))
  # compile model
  opt = Adam(lr=0.0002, beta_1=0.5)
  model.compile(loss=&#39;binary_crossentropy&#39;, optimizer=opt, metrics=[&#39;accuracy&#39;])
  return model

# Input is given from real dataset i,e. MNIST handwritten digit images for training, it has to predict whether the image generated is real or fake.

# Code to define generator
def define_generator(latent_dim):
  model = Sequential()
  # foundation for 7x7 image
  n_nodes = 128 * 7 * 7
  model.add(Dense(n_nodes, input_dim=latent_dim))
  model.add(LeakyReLU(alpha=0.2))
  model.add(Reshape((7, 7, 128)))
  # upsample to 14x14
  model.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding=&#39;same&#39;))
  model.add(LeakyReLU(alpha=0.2))
  # upsample to 28x28
  model.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding=&#39;same&#39;))
  model.add(LeakyReLU(alpha=0.2))
  model.add(Conv2D(1, (7,7), activation=&#39;sigmoid&#39;, padding=&#39;same&#39;))
  return model

# It takes input as points from latent space and generated images which are similar to the real dataset images and sends it to discriminator expecting it to predict it to be true real image.

# Training the model
def train(g_model, d_model, gan_model, dataset, latent_dim, n_epochs=100,
n_batch=256):
  bat_per_epo = int(dataset.shape[0] / n_batch)
  half_batch = int(n_batch / 2)
  # manually enumerate epochs
  for i in range(n_epochs):
  # enumerate batches over the training set
  for j in range(bat_per_epo):
    # get randomly selected &#39;real&#39; samples
    X_real, y_real = generate_real_samples(dataset, half_batch)
    # generate &#39;fake&#39; examples
    X_fake, y_fake = generate_fake_samples(g_model, latent_dim,
    half_batch)
    # create training set for the discriminator
    X, y = vstack((X_real, X_fake)), vstack((y_real, y_fake))
    # update discriminator model weights
    d_loss, _ = d_model.train_on_batch(X, y)
    # prepare points in latent space as input for the generator
    X_gan = generate_latent_points(latent_dim, n_batch)
    # create inverted labels for the fake samples
    y_gan = ones((n_batch, 1))
    # update the generator via the discriminator&#39;s error
    g_loss = gan_model.train_on_batch(X_gan, y_gan)
    # summarize loss on this batch
    print(&#39;&gt;%d, %d/%d, d=%.3f, g=%.3f&#39; % (i+1, j+1, bat_per_epo,
    d_loss, g_loss))
    # evaluate the model performance, sometimes
  if (i+1) % 10 == 0:
    summarize_performance(i, g_model, d_model, dataset, latent_dim)
